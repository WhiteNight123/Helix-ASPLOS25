# Simulator cluster file for 2 RTX2080Ti + 1 RTX4090 GPUs running Qwen3-32B-AWQ
# RTX2080Ti GPUs: 16 layers each (32 total layers)
# RTX4090 GPU: 32 layers
# AWQ quantization reduces memory footprint

[Coordinator]
inbound_nic_speed=10000.0 * mbps
outbound_nic_speed=10000.0 * mbps

[MachineTypes]
types=['RTX2080Ti', 'RTX4090']

[ComputeNodes]
names=['compute_node_2', 'compute_node_3', 'compute_node_4']

[compute_node_2]
vram_size=11000.0 * MB
inbound_nic_speed=10000.0 * mbps
outbound_nic_speed=10000.0 * mbps
disk_speed=1000.0 * mbps
machine_type="RTX2080Ti"
kv_cache_capacity=4480000
activation_backup_capacity=0

[compute_node_3]
vram_size=11000.0 * MB
inbound_nic_speed=10000.0 * mbps
outbound_nic_speed=10000.0 * mbps
disk_speed=1000.0 * mbps
machine_type="RTX2080Ti"
kv_cache_capacity=4480000
activation_backup_capacity=0

[compute_node_4]
vram_size=24000.0 * MB
inbound_nic_speed=10000.0 * mbps
outbound_nic_speed=10000.0 * mbps
disk_speed=1000.0 * mbps
machine_type="RTX4090"
kv_cache_capacity=10240000
activation_backup_capacity=0

[Links]
names=['link_source_compute_node_2', 'link_compute_node_2_compute_node_3', 'link_compute_node_3_compute_node_4', 'link_compute_node_4_sink']

[link_source_compute_node_2]
in=source
out=compute_node_2
latency=1.0 * MilliSec
bandwidth=10000.0 * mbps

[link_compute_node_2_compute_node_3]
in=compute_node_2
out=compute_node_3
latency=1.0 * MilliSec
bandwidth=10000.0 * mbps

[link_compute_node_3_compute_node_4]
in=compute_node_3
out=compute_node_4
latency=1.0 * MilliSec
bandwidth=10000.0 * mbps

[link_compute_node_4_sink]
in=compute_node_4
out=sink
latency=1.0 * MilliSec
bandwidth=10000.0 * mbps
